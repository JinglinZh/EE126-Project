# EE126-Project

This project is referenced from [the instruction of EE126 project](https://inst.eecs.berkeley.edu/~ee126/fa18/project_proposal.pdf).

## Project Proposal: Simulate a Simplified Game of Catan

Our team members propose the following two approaches to design the action policy to win the game. After the experiment, we will compare the results and choose the better one as the final policy.

### 1. Approach by DRL
The game of Catan can not only be described by a large number of variables which give rise to large state spaces but also potentially generate a large action space due to the choice of various resources. Methods of optimizing the interaction by compression do not perform very well since people fail to extract features that can represent spaces. Therefore, this encourages us to apply the Deep Reinforcement Learning (DRL) where agent can simultaneously learn useful features and eventually acquire optimal policy. 

We assume state space $\mathcal{S}$, action space $\mathcal{A}$ and transition operator $\mathcal{T}$. A reward function $r$ is given by $r(s, a, s')$ when the agent chooses action $a$ with the transition of state from $s$ to $s'$. In order to select the best action, we can maximize the expectation of the sum of rewards discounted by $\gamma$ at each time step over polices of $\pi$, i.e., $Q^{*}(s,a) = \max_{\pi}\mathbb{E}[r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} + \cdots|s_{t} = s, a_{t} = a, \pi]$ and then the agent will take the best action at test by $\max_{a}p(a|s)$.

In terms of $Q^{*}$, we intend to train a convolutional neural network (CNN). Assume that we have the set of experience $D = \{e_{1}, \dots, e_{N}\}$, where $e_{t} = (s_{t}, a_{t}, r_{t}, r_{t+1})$. Further, with respect to the Q-learning updates over the minibatches of experience, we use the loss function as $L_{i}(\theta_{i}) = \mathbb{E}_{\mathcal{B}}[(r + \gamma\max_{a'}Q(s',a';\bar{\theta_{i}}) - Q(s,a;\theta_{i}))^{2}]$, where $\theta_{i}$ and $\bar{\theta_{i}}$ are the parameters of the NN and the target ones at iteration $i$ respectively and $\mathcal{B}$ is the minibatch drawn from $D$ uniformly randomly.

Firstly, constrained by the implementing rules, our policy should be based on the particular state of the game and resources available to obtain or spend. Secondly, the agent also has to response to the action made by the opponent. We intend to join these two tasks together to reach completeness. Also note learning from the large actions sets is probably time-expensive, instead of learning from whole action sets, learning from the constrained sets can be helpful to address this issue.

### 2. Approach by Markov Chain
When playing the game of Catan, our method is to try to maximize the profit of the next step. For every step we can choose to construct buildings, trade for other resources and buy card for points. The method will decide what to do base on the potential profit the choice can brought to us. We will develop different evaluation function to get the expectation of the profit of each action.

We generate the function of evaluation as $V = {\alpha T}+{\beta P}+{\delta N}+{\sigma R}$ where $T$ is the expected time needed before we can make the move, $P$ is the points we already have, $N$ is the potential new resources or points generated by the move and $R$ is the resources cost to make the move.

We make three Markov Chains to calculate the expected time to obtain enough resources to build settlements, upgrade a settlement to a city and buy a victory point respectively. Take the Markov Chain of card for example, you have 3 wood, 0 brick, 0 grain and a settlement requires 2 wood, 1 brick, 1 grain. Thus the distance from your state to buying a card is 1 brick and 1 grain. When calculating the expected time to build a settlement, we use the position that decided by the position choosing process, the spent to build that settlement is the sum of spent to build both the road and the settlement.

When deciding the position to build a settlement, there are three aspects that should be taken into consideration: the difficulty to build the specific settlement which is measured by the minimum length of road in order to reach the position of settlement, the difficulty of opponents to interfere our building, the potential profit that settlement can make in the future. The potential profit is evaluated by $\sum P*N$ where $P$ denotes the probability of getting the resource and $N$ is the number of resources got from the land.


The difficulty to build a settlement is decided by our current resources and the expectation of resources gain from current situation. Similarly, we could get the difficulty for opponents to build the same settlement. $D = \alpha C+\beta P$, where $C$ denotes resources we already have and $P$ denotes the potential resources we can get from what we already have, $D$ is the difficulty to build a settlement.